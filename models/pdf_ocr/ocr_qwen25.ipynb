{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c940433-f301-4e4d-bb1d-2cab4cd4f619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. üìÑ PDF information Extraction\n",
    "### VQA based on QWEN2 VLM model for scanned PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pdfplumber\n",
    "import fitz \n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735680b5-64a0-4b0e-a41c-ea1b700132ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/farzaneh/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./../../../../data/sample_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pdf_path_files): 37\n",
      "Ziad Abdeltawab\n",
      "./../../../../data/sample_resume/Ziad Abdeltawab.pdf\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_path='./../../../../data/sample_resume'):\n",
    "    \"\"\"\n",
    "    Finds all .pdf files in the given relative directory and returns their relative paths\n",
    "    and file names without the .pdf extension.\n",
    "\n",
    "    Parameters:\n",
    "        data_path (str): Relative path to the directory to search for PDF files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: \n",
    "            - List of relative paths to PDF files.\n",
    "            - List of PDF file names without the .pdf extension.\n",
    "    \"\"\"\n",
    "    pdf_path_files = []\n",
    "    pdf_names = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                relative_path = os.path.join(root, file)\n",
    "                pdf_path_files.append(relative_path)\n",
    "                pdf_name = os.path.splitext(file)[0]\n",
    "                pdf_names.append(pdf_name)\n",
    "    \n",
    "    return pdf_path_files, pdf_names\n",
    "\n",
    "pdf_path_files, pdf_names = read_data()\n",
    "print(f'len(pdf_path_files): {len(pdf_path_files)}')\n",
    "\n",
    "print(pdf_names[2])\n",
    "print(pdf_path_files[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd4caa9-8d9f-4f55-9f4f-5083aa6a220a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA using QWEN-VLM model for scanned PDF docs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor loaded\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class PDFDataExtractionViaVLM:\n",
    "  def __init__(self):\n",
    "    print('VQA using QWEN-VLM model for scanned PDF docs ...')\n",
    "    # load the QWEN VLM model\n",
    "    self.load_model()\n",
    "\n",
    "    # load the QWEN VLM processor\n",
    "    self.load_model_processor()\n",
    "\n",
    "    # create ./temp directory\n",
    "    os.makedirs('./temp/', exist_ok=True)\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def pdf2image(pdf_path, page_number=1):\n",
    "    \"\"\"\n",
    "    Converts a specific page of a PDF file into a .png image and displays it.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    \n",
    "    1. Opens the PDF document from the specified path.\n",
    "    2. Loads the specified page (by default, the first page) of the PDF.\n",
    "    3. Converts the page to an image (PNG format) and displays it.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        page_number (int, optional): The page number to convert, starting from 0. Defaults to the second page.\n",
    "    \n",
    "    Returns:\n",
    "        Image: The converted image object that is also saved locally as ./temp/temp_image.png'.\n",
    "    \"\"\"\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Select the specific page\n",
    "    page = pdf_document.load_page(page_number)\n",
    "\n",
    "    # Render the page to a PNG image\n",
    "    pix = page.get_pixmap()\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "\n",
    "    # # Create an image from the bytes and display it\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    # display(image)  # Use display() for Databricks notebooks\n",
    "\n",
    "    # Save the image to a file\n",
    "    image_path = './temp/temp_image.png'\n",
    "    image.save(image_path)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def display_pdf_doc(pdf_path):\n",
    "    \"\"\"\n",
    "    Converts a specific page of a PDF file into a .png image and displays it.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    \n",
    "    1. Opens the PDF document from the specified path.\n",
    "    2. Loads the specified page (by default, the first page) of the PDF.\n",
    "    3. Converts the page to an image (PNG format) and displays it.\n",
    "    4. Saves the converted image to a temporary file named./temp/temp_image.png'.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        page_number (int, optional): The page number to convert, starting from 0. Defaults to the first page.\n",
    "    \n",
    "    Returns:\n",
    "        Image: The converted image object that is also saved locally as./temp/temp_image.png'.\n",
    "    \"\"\"\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    dispalay_page = True\n",
    "    page_number = 0\n",
    "    while dispalay_page:\n",
    "      try:\n",
    "        # Select the specific page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "\n",
    "        # Render the page to a PNG image\n",
    "        pix = page.get_pixmap()\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "\n",
    "        # Create an image from the bytes and display it\n",
    "        image = Image.open(io.BytesIO(img_data))\n",
    "        # print(f'page number: {page_number}')\n",
    "        display(image)  # Use display() for Databricks notebooks\n",
    "        page_number += 1\n",
    "\n",
    "      except ValueError:\n",
    "        dispalay_page = False\n",
    "\n",
    "\n",
    "  def load_model(self):\n",
    "      \"\"\"\n",
    "      Loads the Qwen2-VL model from the specified local directory or downloads it\n",
    "      to that directory if not available locally.\n",
    "\n",
    "      The function performs the following steps:\n",
    "      \n",
    "      1. Attempts to load the model from a local directory (`model_path`).\n",
    "      2. If the model is not found locally, it downloads the model from Hugging Face,\n",
    "        caches it in the specified cache directory, and saves it to model_path.\n",
    "      3. Returns the loaded model instance, which is configured to use GPU (CUDA).\n",
    "\n",
    "      Args:\n",
    "          None\n",
    "\n",
    "      Returns:\n",
    "          model (Qwen2_5_VLForConditionalGeneration): The loaded Qwen2-VL model.\n",
    "      \"\"\"\n",
    "      model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "      model_path = \"./../../../../hf_models/qwen_vlm_model\"\n",
    "      cache_dir = \"./../../../../hf_models/cache\"\n",
    "      \n",
    "      # Create directories if they don't exist\n",
    "      os.makedirs(model_path, exist_ok=True)\n",
    "      os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "      try: \n",
    "          # Try to load from local model_path first\n",
    "          model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "              model_path, \n",
    "              torch_dtype=\"auto\", \n",
    "              device_map=\"cuda\",\n",
    "              local_files_only=True\n",
    "          ) \n",
    "          print(\"Model loaded from local directory.\")\n",
    "\n",
    "      except: \n",
    "          print(\"Model not found locally. Downloading from Hugging Face...\")\n",
    "          \n",
    "          # Download from Hugging Face with custom cache directory\n",
    "          model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "              model_id, \n",
    "              torch_dtype=\"auto\", \n",
    "              device_map=\"cuda\",\n",
    "              cache_dir=cache_dir\n",
    "          ) \n",
    "\n",
    "          # Save the model to model_path for future use\n",
    "          model.save_pretrained(model_path) \n",
    "          print(f\"Model downloaded, cached to {cache_dir}, and saved to {model_path}\")\n",
    "\n",
    "      self.model = model\n",
    "\n",
    "\n",
    "  def load_model_processor(self):\n",
    "    model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    self.processor = processor\n",
    "\n",
    "    print('processor loaded')\n",
    "    \n",
    "\n",
    "\n",
    "  def inference(self, prompt, image_local_path=\"./temp/temp_image.png\", sys_prompt=\"You are a helpful assistant.\", max_new_tokens=4096, return_input=False):\n",
    "      image = Image.open(image_local_path)\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "          {\"role\": \"user\", \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": prompt},\n",
    "                  {\"image\": image_local_path},\n",
    "              ]\n",
    "          },\n",
    "      ]\n",
    "      text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "      print(\"text:\", text)\n",
    "      # image_inputs, video_inputs = process_vision_info([messages])\n",
    "      inputs = self.processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "      inputs = inputs.to('cuda')\n",
    "\n",
    "      output_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "      generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "      output_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "      if return_input:\n",
    "          return output_text[0], inputs\n",
    "      else:\n",
    "          return output_text[0]\n",
    "        \n",
    "  @staticmethod  \n",
    "  def get_no_pages_in_pdf(pdf_path):\n",
    "    pdf = pdfplumber.open(pdf_path)\n",
    "    return len(pdf.pages)\n",
    "\n",
    "\n",
    "  def __call__(self, file_path, prompt, verbose=False):\n",
    "      # for .pdf file\n",
    "      if file_path.lower().endswith('.pdf'):\n",
    "          \n",
    "        # get the number of pages in the pdf\n",
    "          pdf_path = file_path\n",
    "          no_pages_in_pdf = self.get_no_pages_in_pdf(pdf_path)\n",
    "          if verbose:\n",
    "            print(f'no_pages_in_pdf: {no_pages_in_pdf}')\n",
    "            self.display_pdf_doc(pdf_path)\n",
    "          pdf_content = []\n",
    "\n",
    "          # extract textual and tabular content of a page\n",
    "          for page_number in range(no_pages_in_pdf):\n",
    "              pdf_content.append(f'\\n\\npage_number: {page_number}\\n')\n",
    "              \n",
    "              # convert the page to iamge and save that image locally\n",
    "              self.pdf2image(pdf_path, page_number)\n",
    "              \n",
    "              # call inference method to ocr the image\n",
    "              output_text = self.inference(prompt, image_local_path=\"./temp/temp_image.png\")\n",
    "              pdf_content.append(output_text)\n",
    "          \n",
    "          return ''.join(pdf_content)\n",
    "      \n",
    "      # for images\n",
    "      else:\n",
    "         output_text = self.inference(prompt, image_local_path=file_path)\n",
    "         return output_text\n",
    "\n",
    "      \n",
    "\n",
    "pdf_data_extraction_via_vlm = PDFDataExtractionViaVLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63eb353a-f7f4-4427-8c0a-b74f7121616d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "usage example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb2cdb3-7ad2-4bf0-8659-ca12a358e4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = pdf_path_files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5c13b2-23a4-4a14-93b7-b9a4a72be88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "no_pages_in_pdf = pdf_data_extraction_via_vlm.get_no_pages_in_pdf(pdf_path)\n",
    "print(no_pages_in_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95a2fe9-4425-4f27-b997-2f51317992c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# qwen 2.5\n",
    "# prompt = \"Read all the text in the image.\"\n",
    "# prompt = \"Please output only the text content from the image without any additional descriptions or formatting.\n",
    "prompt = \"Please output only the text content from the image without any additional descriptions or formatting. the image is a page of a professional resume. there are different sections in each image such as experience or projects. put together all the information about a section or topic together. separate each section with a '\\n\\n'.\"\n",
    "# prompt = \"Please output only the text content from the image without any additional descriptions or formatting. ouptut selected and not selected checkmarks\"\n",
    "# prompt = \"Please output only the text content from the image without any additional descriptions or formatting. if checkmarks exists, include selected checkmarks (‚òí) and not selected checkmarks (‚òê)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0594ea30-98af-4075-9f08-5b75c9704c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pdf_content = pdf_data_extraction_via_vlm(pdf_path, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6048531f-dccb-4a09-a005-f48ed58990ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to keep the ocr outputs\n",
    "pdf_files_contents = pd.DataFrame({\n",
    "    'pdf_path': pdf_path_files,\n",
    "    'pdf_name': pdf_names,\n",
    "    'pdf_content': [''] * len(pdf_path_files)  # Initialize with empty strings\n",
    "})\n",
    "\n",
    "# display(pdf_path_files_contents.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please output only the text content from the image without any additional descriptions or formatting. the image is a page of a professional resume. there are different sections in each image such as experience or projects. put together all the information about a section or topic together. separate each section with a '\n",
      "\n",
      "'.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "text: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please output only the text content from the image without any additional descriptions or formatting. the image is a page of a professional resume. there are different sections in each image such as experience or projects. put together all the information about a section or topic together. separate each section with a '\n",
      "\n",
      "'.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(pdf_files_contents.shape[0]):\n",
    "\n",
    "    pdf_path = pdf_files_contents.at[idx, 'pdf_path']\n",
    "\n",
    "    prompt = \"Please output only the text content from the image without any additional descriptions or formatting. the image is a page of a professional resume. there are different sections in each image such as experience or projects. put together all the information about a section or topic together. separate each section with a '\\n\\n'.\"\n",
    "    pdf_content = pdf_data_extraction_via_vlm(pdf_path, prompt)\n",
    "    pdf_files_contents.at[idx, 'pdf_content'] = pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_files_contents_path = './../../data/pdf_files_contents.csv'\n",
    "# # Save to CSV\n",
    "# pdf_files_contents.to_csv(pdf_files_contents_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "complete_pdf_IR_pipeline1.1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "farzaneh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
